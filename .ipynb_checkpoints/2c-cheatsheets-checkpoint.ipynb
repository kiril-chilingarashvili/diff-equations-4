{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf149a3-d053-48b5-aaaf-251d53b18abf",
   "metadata": {},
   "source": [
    "# Unit 2: Laplace transform, Part c - Rules and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25bf686-f96c-4772-a481-7a7e01d04663",
   "metadata": {},
   "source": [
    "## L6.2. Review.\n",
    "\n",
    "### The Laplace transform carries functions of time, $f(t)$ for $t\\geq 0$, to complex functions of a complex variable $s$. The integral definition is\n",
    "## $$ \\mathcal{L} (f(t);s) = \\int_{0}^\\infty f(t) e^{-st} dt $$\n",
    "### Laplace transform technique depends upon developing two lists: A list of ***rules*** and a list of ***computations***. We will add a lot more entries today. So far, these two lists look like this.\n",
    "\n",
    "### ***Rules***.\n",
    "### 1. ***Linearity***.\n",
    "## $$ af(t)+bg(t) \\rightsquigarrow a F(s) + b G(s) $$\n",
    "### 2. ***Inverse transform***. $F(s)$ essentially determines $f(t)$.\n",
    "### 3. ***$t$-derivative rules***. For a function  of exponential type,\n",
    "## $$ f'(t) \\rightsquigarrow sF(s) - f(0) $$\n",
    "### and\n",
    "## $$ f^{(n)}(t) \\rightsquigarrow s^n F(s) - \\left( f(0) s^{n-1} + f'(0) s^{n-2} + \\cdots + f^{(n-1)} (0)  \\right) $$\n",
    "\n",
    "### ***Calculations***.\n",
    "## $$ \\boxed{\\begin{array} {rclr} 1 & \\rightsquigarrow & \\displaystyle \\frac{1}{s} & \\operatorname{ Re}(s)>0 \\\\ e^{rt} & \\rightsquigarrow & \\displaystyle \\frac{1}{s-r} & \\operatorname{ Re}(s)>\\operatorname{ Re}(r) \\\\ \\cos(\\omega t) & \\rightsquigarrow & \\displaystyle \\frac{s}{s^2+\\omega^2} & \\operatorname{ Re}(s)>0 \\\\ \\sin(\\omega t) & \\rightsquigarrow & \\displaystyle  \\frac{\\omega}{s^2+\\omega^2} & \\operatorname{ Re}(s)>0 \\\\ t & \\rightsquigarrow & \\displaystyle \\frac{1}{s^2} & \\operatorname{ Re}(s)>0  \\end{array}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668bdb9b-0407-49e2-862b-020e25d25617",
   "metadata": {},
   "source": [
    "## L6.3. s-derivative rule.\n",
    "\n",
    "### We definitely need to expand our table of calculations: at this point we don't even know how to solve $\\ddot{x} = 1$ with rest initial conditions! It leads to $\\displaystyle s^2X = \\frac{1}{s}$, and we don't have $\\displaystyle \\frac{1}{s^3}$ anywhere in our table. It's not too different from $\\displaystyle \\frac{1}{s^2}$, though; in fact\n",
    "## $$ \\frac{1}{s^3} = -\\frac{1}{2}\\frac{d}{ds}\\frac{1}{s^2} $$\n",
    "### So let's go back to the integral definition and try to recognize what $F'(s)$ is in general:\n",
    "## $$ \\begin{array} {rcl} F'(s) & = & \\displaystyle \\frac{d}{ds} \\int_{0}^\\infty f(t) e^{-st} dt \\\\ \\, & = & \\displaystyle \\int_0^\\infty f(t) \\frac{d}{ds} e^{-st}dt \\\\ \\, & = & \\displaystyle \\int_0^\\infty f(t) (-t) e^{-st} dt = - \\mathcal{L} (tf(t);s) \\end{array} $$\n",
    "### This gives us a new entry in our table of rules:\n",
    "\n",
    "### ***The $t$-derivative rule***.\n",
    "## $$ tf(t) \\rightsquigarrow  -F'(s) $$\n",
    "### In general, this rule becomes\n",
    "## $$ t^n f(t) \\rightsquigarrow (-1)^n F^{(n)}(s) $$\n",
    "### For example, starting with $1 \\rightsquigarrow s^{-1}$ we find, in sequence,\n",
    "## $$ \\begin{array} {rcl} 1 & \\rightsquigarrow & s^{-1} \\\\ t = t\\cdot 1 & \\rightsquigarrow & \\displaystyle -\\frac{d}{ds} s^{-1} = s^{-2} \\\\ t^2 = t \\cdot t & \\rightsquigarrow  & \\displaystyle - \\frac{d}{ds} s^{-2} = 2s^{-3} \\\\ t^3=t\\cdot t^2 & \\rightsquigarrow  & \\displaystyle -\\frac{d}{ds} (2s^{-3}) = 3 \\cdot 2s^{-4} \\end{array} $$\n",
    "### and in general\n",
    "## $$ t^n \\rightsquigarrow \\frac{n!}{s^{n+1}}, \\quad n=0,1,2,\\ldots $$\n",
    "### Note that these all converge for $\\operatorname{Re}(s) > 0$.\n",
    "\n",
    "### So, to return to $\\ddot{x}=1$ with rest initial conditions, we find\n",
    "## $$ \\begin{array} {rcl} s^2X & = & \\displaystyle \\frac{1}{s} \\\\ X & = & \\displaystyle \\frac{1}{s^3} \\\\ x & = & \\displaystyle \\frac{1}{2} t^2 \\end{array} $$\n",
    "### which is indeed the solution to $\\ddot{x} = 1$ with rest initial conditions $x(0) = 0, \\dot{x}(0)=0 $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b883c416-58db-4b91-b8df-9b639de0beee",
   "metadata": {},
   "source": [
    "## L6.4. Resonance.\n",
    "\n",
    "### ***Resonance*** occurs when the input frequency is close to a natural frequency of the system. In that case the periodic system response will have exceptionally large amplitude. In the complete absence of damping (a mathematical possibility) there is no periodic response if the two frequencies coincide.\n",
    "\n",
    "### ***Example 4.1***\n",
    "### Suppose we are driving a spring system through the spring, and there is negligible damping: say\n",
    "## $$ \\ddot{x}+9x=9\\cos(3t) $$\n",
    "### Because the input frequency coincides with the natural frequency of the harmonic oscillator, we are in resonance. There are still solutions, but none of them are periodic. One can be found using complex replacement and the generalized ERF. But we can also find solutions using Laplace transform. As usual with the Laplace transform method, we need to specify initial conditions; let's say that $x(0)=2, \\dot{x}(0)=0$.\n",
    "\n",
    "### Apply $\\mathcal{L} $ to both sides, and remembering the terms arising from the initial condition:\n",
    "## $$ \\begin{array} {rcl} 9\\cos(3t) & \\rightsquigarrow & \\displaystyle \\frac{9s}{s^2+9} \\\\ x & \\rightsquigarrow & X \\\\ \\ddot{x} & \\rightsquigarrow & s^2 X - s x(0) - x'(0) = s^2X-2s \\end{array} $$\n",
    "### we get the algebraic equation in $X$:\n",
    "## $$ (s^2+9)X - 2s = \\frac{9s}{s^2+9} $$\n",
    "### or\n",
    "## $$ X = \\frac{9s}{(s^2+9)^2} + \\frac{2s}{s^2+9} $$\n",
    "### This is already in standard partial fractions form! We need to find the inverse Laplace transform of this expression. The second term is easy, but the first is not in our table. But it is very close to the derivative (with respect to $s$) of $(s^2+9)^{-1}$, which is a known Laplace transform. So let's use the $s$-derivative rule: Since\n",
    "## $$ \\sin(\\omega t) \\rightsquigarrow \\frac{\\omega}{s^2+\\omega^2} $$\n",
    "### we have\n",
    "## $$ \\begin{array} {rcl} t\\sin(\\omega t) & \\rightsquigarrow  & \\displaystyle -\\frac{d}{ds} \\frac{\\omega}{s^2+\\omega^2} \\\\ \\, & = & \\displaystyle \\frac{2\\omega s}{(s^2+\\omega^2)^2} \\end{array} $$ \t\t\t \t \n",
    "### Returning to our original problem, with $\\omega=3$,\n",
    "## $$ \\frac{3}{2} t\\sin(3t) \\rightsquigarrow \\frac{9s}{(s^2+\\omega^2)^2} $$\n",
    "### and we find\n",
    "## $$ x = \\frac{3}{2} t \\sin(3t) + 2\\cos(3t) $$\n",
    "### Just as a check, compute that $x(0) = 2$ and $\\dot{x}(0) = 0$.\n",
    "\n",
    "### The presence of a repeated factor in the denominator of the Laplace transform $X(s)$ is the mark of resonance in the $s$-domain. As you see, in this example one factor came from a characteristic polynomial of the system, the other from the Laplace transform of the input signal. Repeated factors produce repeated roots, and when they occur in the denominator they produce poles of “higher multiplicity\".\n",
    "\n",
    "### ***The $s$-derivative rule and the multiplicity of poles***:\n",
    "## $$ t f(t) \\rightsquigarrow -F'(s) $$\n",
    "### By iterating this, we find\n",
    "## $$ t^k f(t) \\rightsquigarrow (-1)^k F^{(k)}(s) $$\n",
    "### For example, since $e^{rt} \\rightsquigarrow (s-r)^{-1}$,\n",
    "## $$ t^{k-1}e^{rt} \\rightsquigarrow \\frac{(k-1)!}{(s-r)^k} $$\n",
    "### The $s$-derivative rule associates a pole at $s=r$ of multiplicity $k$ with a polynomial of degree $k-1$ times $e^{rt}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f793fb-4196-466b-afd7-9ff19fba72c8",
   "metadata": {},
   "source": [
    "## L6.5. More computations with quadratic denominators.\n",
    "\n",
    "### The previous example used the $s$-derivative to compute $\\mathcal{L} (t\\sin(\\omega t); s)$. The $s$-derivative rule also leads to a computation of $\\mathcal{L} (t\\cos(\\omega t); s)$, and we have two new calculations.\n",
    "## $$ \\boxed{\\begin{array} {rcl} t\\sin(\\omega t) & \\rightsquigarrow & \\displaystyle \\frac{2\\omega s}{(s^2+\\omega^2)^2} \\\\ t\\cos(\\omega t) & \\rightsquigarrow & \\displaystyle \\frac{s^2-\\omega^2}{(s^2+\\omega^2)^2} \\end{array}} $$\n",
    "### Because we are often using our Laplace tables to go backwards, that is to compute $\\mathcal{L}^{-1}$, it would be useful to have $\\displaystyle \\mathcal{L}^{-1}\\left( \\frac{1}{(s^2+\\omega^2)^2};t \\right)$ on our table as well.\n",
    "\n",
    "### We also can derive:\n",
    "## $$ \\boxed{ \\frac{1}{2\\omega} \\left( \\frac{1}{\\omega} \\sin(\\omega t) - t \\cos(\\omega t) \\right) = \\frac{1}{(s^2+\\omega^2)^2} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d553ef8-31f2-4d80-8b67-2358f3970fd6",
   "metadata": {},
   "source": [
    "## L6.6. The s-shift rule.\n",
    "\n",
    "### We have found the Laplace transform of sinusoidal functions. Damped sinusoids are just as important and occur as transients of an LTI system response, so we should work that out too.\n",
    "\n",
    "### ***Example***\n",
    "### Find the Laplace transform of $e^{at} \\cos(\\omega t)$\n",
    "\n",
    "### ***Solution***\n",
    "### Recall that\n",
    "### 1. the complex conjugate of $e^{i\\omega t}$ is $e^{-i\\omega t}$, and\n",
    "### 2. $\\displaystyle \\operatorname{Re}(z) = \\frac{z+\\overline{z}}{2}$\n",
    "### Euler's formula gives\n",
    "## $$ \\cos(\\omega t) = \\operatorname{ Re}(e^{i\\omega t}) $$\n",
    "### the exponential rule then gives\n",
    "## $$ e^{at}\\cos(\\omega t) = \\frac{1}{2}\\left( e^{(a+i\\omega)t} + e^{(a-i\\omega)t} \\right) $$\n",
    "### We've computed that\n",
    "## $$ e^{rt} \\rightsquigarrow \\frac{1}{s-r} $$\n",
    "### so\n",
    "## $$ e^{(a+i\\omega)t} \\rightsquigarrow \\frac{1}{s-(a+i\\omega)} $$\n",
    "### The real part of this expression is the answer:\n",
    "## $$ \\operatorname{ Re} \\left( \\frac{1}{s-(a+i\\omega)} \\right) = \\operatorname{ Re} \\left( \\frac{1}{(s-a)-i\\omega} \\right) =  \\operatorname{ Re} \\left( \\frac{(s-a) + i\\omega}{(s-a)^2+\\omega^2} \\right) $$\n",
    "### namely\n",
    "## $$ \\boxed{e^{at} \\cos(\\omega t) \\rightsquigarrow \\frac{s-a}{(s-a)^2+\\omega^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742d721e-87d6-485a-ac17-02f17093d47f",
   "metadata": {},
   "source": [
    "### A similar calculation to the previous problem, using\n",
    "## $$ \\sin(\\omega t) = \\operatorname{Im}(e^{i\\omega t}) $$\n",
    "### gives us the Laplace transform\n",
    "## $$ e^{a t} \\sin(\\omega t) \\rightsquigarrow \\frac{\\omega}{(s-a)^2+\\omega^2} $$\n",
    "### By forming linear combinations, we can find the Laplace transform of any damped sinusoid. But the form of the answer here gives us pause; it's very close to the Laplace transform of the ***undamped*** sinusoid $\\cos(\\omega t)$. Let's see if this is something general. So suppose we know $f(t) \\rightsquigarrow F(s)$, and use the integral expression for the Laplace transform to compute\n",
    "## $$ \\begin{array} {rcl} \\mathcal{L} (e^{at} f(t); s) & = & \\displaystyle \\int_{0}^\\infty (e^{at}f(t)) e^{-st} dt \\\\ \\, & = & \\displaystyle \\int_0^\\infty f(t) e^{-(s-a)t} \\end{array} $$\n",
    "### That is,\n",
    "\n",
    "### ***The $s$-shift rule***:\n",
    "## $$ \\boxed{e^{at}f(t) \\rightsquigarrow F(s-a)} $$\n",
    "### This is consistent with the calculation we just did, and shows also that\n",
    "## $$ \\boxed{e^{at} \\sin(\\omega t) \\rightsquigarrow \\frac{\\omega}{(s-a)^2+\\omega^2}} $$\n",
    "\n",
    "### ***Consistency***.\n",
    "\n",
    "### It is a good exercise to check for consistency among our various formulas:\n",
    "### We have $\\displaystyle \\mathcal{L} (1) = \\frac{1}{s}$, so the $s$-shift formula gives $\\displaystyle \\mathcal{L} (e^{at}\\cdot 1) = \\frac{1}{s-a}$. This matches our formula for $\\mathcal{L} (e^{at})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf3059f-202f-4b43-af72-3bbd6e5b7fef",
   "metadata": {},
   "source": [
    "### So far we learned the following ***rules***:\n",
    "## $$ \\boxed{\\begin{array} {lclr} f'(t) & \\rightsquigarrow & sF(s) - f(0), & t\\text{-derivative rule} \\\\ t f(t) & \\rightsquigarrow & -F'(s), & s\\text{-derivative rule} \\\\ e^{at}f(t) & \\rightsquigarrow & F(s-a),& s\\text{-shift rule} \\end{array}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b7d55-0812-440d-b433-9d99c31f7fee",
   "metadata": {},
   "source": [
    "## L6.7. The Pole Diagram and Homogeneous Solutions.\n",
    "\n",
    "### Previously, we learned how to read stability information of an LTI system from the pole diagram of its transfer function. We also learned how to take the Laplace Transform of a function $f(t)$ in the time domain, and express it instead in the frequency domain, $F(s)$. This gave us a new method to find a system response of an LTI system to a wide variety of input signals using algebra, partial fractions, and inverse Laplace Transforms read off of a table.\n",
    "\n",
    "### In practice, the real power of Laplace transform is that it gives you very useful information about the solutions without requiring you to solve it all out explicitly. In particular, ***the pole diagram of the transfer function is the pole diagram of a generic zero input response (ZIR)***. (Recall that the zero input response is the solution when the input is $0$, or the solution to the associated homogeneous equation.) This means that important information about the homogeneous solutions can be read off directly from the pole diagram of the transfer function.\n",
    "\n",
    "### - a real pole of $F(s)$ at $s=a$ comes from a term in $f(t)$ of the form $e^{at}$.\n",
    "### - a conjugate pair $a\\pm i\\omega$ comes from a term in $f(t)$ of the form $e^{at}\\cos(\\omega t-\\phi)$.\n",
    "\n",
    "### ***Examples***\n",
    "\n",
    "### 1. An LTI system has the following transfer function:\n",
    "## $$ H(s) = \\frac{Q(s)}{P(s)} = \\frac{1}{(s^2+2s+2)(s+2)} = \\frac{As+B}{s^2+2s+2} + \\frac{C}{s+2} $$\n",
    "### This system is stable since the poles $s=-1\\pm i, -2$ have negative real part.\n",
    "![img](img/sys1.png) \n",
    "### The pole diagram implies that the general zero input response is\n",
    "## $$ A e^{-t} \\cos(t) + B e^{-t} \\sin(t) + C e^{-2t} = D e^{-t} \\cos(t-\\phi) + C e^{-2t} $$\n",
    "### We can read this homogeneous solution immediately off of the pole diagram, which is the pole diagram for the Laplace transform of a generic homogeneous solution.\n",
    "\n",
    "### The poles at $-1\\pm i$ contribute to a transient term $D e^{-t} \\cos(t-\\phi)$. The pole at $s=-2$ contributes to a transient term $C e^{-2t}$.\n",
    "\n",
    "### 2. A certain LTI system has the following pole diagram.\n",
    "![img](img/sys2.png)\n",
    "\n",
    "### From the pole diagram, we see that a generic homogeneous solution for this system takes the form\n",
    "## $$ A_1 e^{-t} + B e^{-2t} + C\\cos(2t) + D\\sin(2t) $$\n",
    "### The first two terms coming from poles in the left half plane are transient and tend to zero as $t$ tends to $\\infty$. The poles along the imaginary axis correspond to\n",
    "## $$ C\\cos(2t) + D\\sin(2t) $$\n",
    "### This system is not stable since the poles do not all have negative real part.\n",
    "\n",
    "### ***Summary of new terminology as it relates to old terminology***\n",
    "### Given an LTI system $P(D)x = Q(D)y$, where $P$ and $Q$ have no common factors. Then the transfer function is $\\displaystyle H(s) = \\frac{Q(s)}{P(s)}$.\n",
    "## $$ \\begin{array} {rcl} \\text{New terminology} & & \\text{Old terminology} \\\\ \\text{Poles of}\\,H(s) & = & \\text{The zeros of}\\,P(D) \\\\ \\text{ZIRs (zero input sources)} & = & \\text{Homogeneous solutions (solutions to}\\,P(D)x=0\\text{)} \\end{array} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498e246-cac2-4e39-8bfb-38b9316dc65b",
   "metadata": {},
   "source": [
    "### Mathlet:\n",
    "\n",
    "### The mathlet below displays the graph of the function\n",
    "## $$ x(t) = Ae^{at}\\cos(bt) + B e^{ct}\\cos(dt) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740f9dd9-058e-4381-afa7-b8590281a771",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"900\" height=\"650\" src=\"https://1803mathlets.netlify.app/polesvibrations\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>  \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"900\" height=\"650\" src=\"https://1803mathlets.netlify.app/polesvibrations\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b97cdda-ba23-4442-8b2d-89f11fd2f723",
   "metadata": {},
   "source": [
    "### Beats mathlet\n",
    "\n",
    "### Now sum 2 sinusoidal oscillations. Set $A=B=2$ to get better resolution, and set $a=c=0$: there is no decay, both $f(t)$ and $g(t)$ are undamped sinusoids. All the poles are on the imaginary axis.\n",
    "\n",
    "### Observe what happens when the frequencies of the two oscillations are far apart: $d=8, b=1$. When the frequencies are close together ($d=7$ and $b=6.5$), you see the phenomenon called beats. You would hear this as “waawaawaa...\" – the amplitude rises and falls. Let's control the time $T$ from one maximum amplitude to the next by adjusting the location of the poles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b056b574-fb07-4c24-a0fc-f2065a68c997",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe width=\"900\" height=\"650\" src=\"https://1803mathlets.netlify.app/polesvibrations\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>  \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<iframe width=\"900\" height=\"650\" src=\"https://1803mathlets.netlify.app/polesvibrations\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf0901-0d55-40ed-9ba8-0c0db10f5c6b",
   "metadata": {},
   "source": [
    "## L6.9. Conclusions on growth and decay.\n",
    "\n",
    "### We can make some conclusions from this investigation. Most important:\n",
    "### 1. ***$f(t)$ decays exponentially to zero exactly when all of the poles of $F(s)$ have negative real part; that is, they all lie in the left half plane***.\n",
    "### To put this in context in our analysis of LTI systems: the homogeneous solutions to an LTI system are transient when the poles of the transfer function of that system all have negative real part.\n",
    "\n",
    "### 2. ***The right-most poles of $F(s)$ determine the long-term behavior of $f(t)$***.\n",
    "### If the real part of some pole is positive, then $f(t)$ grows exponentially as $t\\to\\infty$. But in either case, we can be very precise about the rate of growth or decay.\n",
    "\n",
    "### ***Example 9.1***\n",
    "### What is the exponential type of the function whose Laplace transform has the following pole diagram?\n",
    "![img](img/sys3.png)\n",
    "\n",
    "### ***Solution***: \n",
    "### The dominant term is $Ae^{-t}\\cos(2t-\\phi)$. This function is of exponential type $-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e1280-cf31-4949-bc2c-ccea6b9926c4",
   "metadata": {},
   "source": [
    "### ***Poles with multiplicity***\n",
    "### A further complication is the possibility of a repeated factor in the denominator. This results in a “double pole\" (if the factor occurs squared) or higher in the pole diagram. We have seen the effect of this, too:\n",
    "## $$ t^n \\rightsquigarrow \\frac{n!}{s^{n+1}} $$\n",
    "### so, by the $s$-shift rule,\n",
    "## $$ t^n e^{at} \\rightsquigarrow \\frac{n!}{(s-a)^{n+1}} $$\n",
    "### A similar rule holds for repeated complex roots: an $(n+1)$-fold root produces multiplication of the expected $f(t)$ by $t^n$. This is a general case of “resonance.\"\n",
    "\n",
    "### The main point is that $t^n e^{at}$ is of exponential type $b$ for any $b>a$. Thus $\\mathcal{L} (t^n e^{at};s)$ converges for $\\operatorname{Re}(s)>a$. This is exactly the same region of convergence as for $e^{at}$ which has exponential type $b$ for any $b\\geq a$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b9f499-5355-4b9d-a82b-ae087219cc99",
   "metadata": {},
   "source": [
    "## L6.10. Region of convergence again.\n",
    "\n",
    "### We defined the Laplace transform $F(s) = \\mathcal{L} (f(t);s)$ using an improper integral, and made the point that this integral will generally only converge for $s$ to the right of some vertical line in the complex plane. Now we can understand better why the convergence fails: it fails when you hit a pole. Generally, the region of convergence is the half-plane to the right of the rightmost pole.\n",
    "\n",
    "### On the other hand, we have just been discussing the pole diagram of functions such as $F(s)$, apparently making sense of it outside its region of convergence. What's going on?\n",
    "\n",
    "### The fact is that knowledge of functions such as $F(s)$ in some right half plane suffices to determine a function defined everywhere in the complex plane (except for a scattering of points, the poles). You will learn about this if you take a course in complex variables; the process is known as ***analytic continuation***.\n",
    "\n",
    "### Most of the functions $F(s)$ that arise in this course are actually rational functions. Their expression, as a quotient of one polynomial by another, allows computation of the values of the function for almost every complex number. It provides us with the analytic continuation of the function given by the integral expression for the Laplace transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2504c5-5c58-476b-991f-e8af1063abc5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
